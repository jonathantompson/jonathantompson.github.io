<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
  <title>Jonathan Tompson</title>
  <meta name="description" content="Academic Website">
  <meta name="author" content="Jonathan Tompson">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="./css" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="./normalize.css">
  <link rel="stylesheet" href="./skeleton.css">
  <link rel="stylesheet" href="./custom.css">
  <link rel="stylesheet" href="./social-circles.min.css">


  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="SHORTCUT ICON" href="favicon.ico"/>
  <link rel="apple-touch-icon" href="favicon_apple.ico"/>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

 <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="#about">About</a></li>
          <li class="navbar-item"><a class="navbar-link" href="#pubs">Projects</a></li>
          <li class="navbar-item"><a class="navbar-link" href="#resume">Resume</a></li>
        </ul>
      </div>
    </nav>


  <div class="container">
    <div class="row">
      <div class="one-half column" style="margin-top: 8.0%">
        <img class="bio-image" src="images/IMG_0101_cropped.JPG" alt="">
      </div>
      <div class="one-half column" style="margin-top: 8.0%">
	    <br>
        <h2>Jonathan Tompson</h2>
        <div>
            <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/icon_google_scholar.png" alt="" width="24" height="24"> </p>
            <a class="project-link" href="https://scholar.google.com/citations?user=U_Jw8DUAAAAJ" target="_blank">Google Scholar</a><br>
        </div>
        <div style="clear: left;">
            <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/icon_github.png" alt="" width="24" height="24"> </p>
            <a href="http://github.com/jonathantompson">github</a><br>
        </div>
        <div style="clear: left;">
            <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/icon_linkedin.png" alt="" width="24" height="24"> </p>
            <a class="project-link" href="https://www.linkedin.com/in/jonathan-tompson-26487413/" target="_blank">LinkedIn</a><br>
        </div>
        <div style="clear: left;">            
            <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/icon_mail.png" alt="" width="24" height="24"> </p>
            <a href="https://jonathantompson.github.io/">Email: first dot last at gmail</a>
        </div>
      </div>
    </div>

    <div class="row" style="margin-top: 7.5%" id="about">
      <p> I am a Staff Research Scientist for <a href="https://research.google.com/teams/brain/">Google Brain</a> in Mountain View, CA. My research background covers a wide range of topics: robotics, imitation learning, reinforcement learning, computer vision and graphics, computational fluid dynamics, unsupervised learning, hand and human body tracking and Integrated Circuit design.</p>
    </div>
  </div>



  <div class="container">

    <div class="row" style="margin-top: 2%" id="pubs">
      <h4>Projects</h4>
    </div>
    
        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/realtime_30.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Interactive Language: Talking to Robots in Real Time</b><br>
                          <em>Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence</em><br/>
                          preprint <a href="https://arxiv.org/abs/2210.06407">[paper]</a> <a href="https://interactive-language.github.io/">[website]</a><br/>
                          We present a framework for building interactive, real-time, natural language-instructable robots in the real world.
                        </p>
                </div>
    	</div>    
        
        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/fig5_mc_cvl_qvals.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Contrastive Value Learning: Implicit Models for Simple Offline RL</b><br>
                          <em>Bogdan Mazoure*, Ben Eysenbach*, Ofir Nachum, Jonathan Tompson (* equal)</em><br/>
                          preprint <a href="others/cvl.pdf">[paper]</a> <br/>
                          We propose Contrastive Value Learning (CVL), which learns an implicit, multi-step model of the environment dynamics. CVL outperforms SoTA on offline RL datasets and we show that it is able to incorporate out-of-domain demonstrations while pretraining to perform task transfer.
                        </p>
                </div>
    	</div>
        
    
        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/algo_schema-01_3_only.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Improving Zero-shot Generalization in Offline Reinforcement Learning using Generalized Similarity Functions</b><br>
                          <em>Bogdan Mazoure, Ofir Nachum, Ilya Kostrikov, Jonathan Tompson</em><br/>
                          NeurIPS 2022 <a href="https://arxiv.org/abs/2111.14629">[paper]</a> <br/>
                          Achieved state-of-the-art in the challenging setting of zero-shot offline RL on the Procgen benchmark. Used a theoretically-motivated framework called Generalized Similarity Functions to learn improved state representations leading to more robust policies with improved generalization.
                        </p>
                </div>
    	</div>
    
    
        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/inner_monologue_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Inner Monologue: Embodied Reasoning through Planning with Language Models</b><br>
                          <em>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter</em><br/>
                          CoRL 2022 <a href="https://arxiv.org/abs/2207.05608">[paper]</a> 
                          <a href="https://innermonologue.github.io/">[website]</a> <br/>
                          Building on SayCan, we incorporate multiple feedback sources into the LLM that allows them to more richly process and plan in robotic control scenarios.
                        </p>
                </div>
    	</div>    
        
        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/kitting_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Keynote Talk: Pick and Place at Scale</b><br>
                          <em>Jonathan Tompson</em><br/>
                          ICRA 2022 Workshop: Challenges in Applying Academic Research to Real-World Robotics. <a href="https://docs.google.com/presentation/d/1A_EvHBsOq4MI3MGrM-vyMWDBS2tQgqGy0F_rC1JE8KY/edit?usp=sharing&resourcekey=0-0KsD7SjS42Zm2b9-jv-MwQ">[slides]</a> <a href="https://www.youtube.com/watch?v=V5zVfmf8cMo">[video]</a> <br/>
                          In this talk I cover the challenges of 3 production robotics problems within the Robotics At Google team: Singulation, Kitting and Depalletization. I discuss the practical and engineering challenges of each task, as well as propose open research questions.
                        </p>
                </div>
    	</div>
        
    

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/ebm_sort_crop.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Implicit Behavioral Cloning</b><br>
                          <em>Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, Jonathan Tompson</em><br/>
                          CoRL 2021 <a href="https://arxiv.org/pdf/2109.00137.pdf">[paper]</a> 
                          <a href="https://implicitbc.github.io/">[website]</a> <br/>
                          We show that on real-world robotic policy learning tasks that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs.
                        </p>
                </div>
    	</div>

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/xirl.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>XIRL: Cross-embodiment Inverse Reinforcement Learning</b><br>
                          <em>Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi</em><br/>
                          CoRL 2021 <a href="https://arxiv.org/pdf/2106.03911.pdf">[paper]</a> 
                          <a href="https://x-irl.github.io/">[website]</a><br/>
                          We introduce x-MAGICAL benchmark geared towards cross-embodiment imitation, and we demonstrate an unsupervised reward-learning approach (using TCC) which successfully learns from expert demonstrations from a different agent embodiment.
                        </p>
                </div>
    	</div>

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/fisher_brc.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Offline Reinforcement Learning with Fisher Divergence Critic Regularization</b><br>
                          <em>Ilya Kostrikov, Jonathan Tompson, Rob Fergus, Ofir Nachum</em><br/>
                          ICML 2021 <a href="https://arxiv.org/abs/2103.08050">[paper]</a> <br/>
                          Achieved state-of-the-art on the offline RL D4RL benchmark by proposing a Fisher divergence behavior regularization term to replace CQL's KL constraint.
                        </p>
                </div>
    	</div>

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/demos-cloth-flat-1x.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-conditioned Transporter Networks</b><br>
                          <em>Daniel Seita, Pete Florence, Jonathan Tompson, Erwin Coumans, Vikas Sindhwani, Ken Goldberg, Andy Zeng</em><br/>
                          ICRA 2021 <a href="https://arxiv.org/pdf/2012.03385.pdf">[paper]</a>
                          <a href="https://berkeleyautomation.github.io/bags/">[website]</a> <br/>
                          This work extends Transporter Nets to perform goal-conditioned manipulation of deformable objects. We propose a suite of simulated benchmark tasks ranging from rope manipulation to placing objects in bags.
                        </p>
                </div>
    	</div>

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/teaser_nnclr.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>With a Little Help From My Friends: Nearest-neighbor Contrastive Learning of Visual Representations</b><br>
                          <em>Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman</em><br/>
                          ICCV 2021 <a href="https://arxiv.org/abs/2104.14548">[paper]</a> <br/>
                          Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), achieved state-of-the-art on unsupervised Imagenet, by novel sampling of negatives using nearest neighbors.
                        </p>
                </div>
    	</div>

        <div class="row">
        <div class="five columns" style="margin-top: 2%">
                    <img class="paper-image" src="images/real-assemble-kit.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
                        <p>
                          <b>Transporter networks: Rearranging the Visual World for Robotic Manipulation</b><br>
                          <em>Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Johnny Lee</em><br/>
                          CoRL 2020 <a href="https://arxiv.org/abs/2010.14406">[paper]</a>
                          <a href="https://transporternets.github.io/">[website]</a><br/>
                          We present a novel end-to-end system for data-efficient robotic manipulation (pick-and-place). Transporter Networks learns to map query pick objects with their target place location.
                        </p>
                </div>
    	</div>

	<div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/counting_1.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Counting Out Time: Class Agnostic Video Repetition Counting in the Wild</b><br>
			  <em>Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman</em><br/>
                          CVPR 2020 <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf">[paper]</a> <a href="https://sites.google.com/corp/view/repnet">[website]</a><br/>
			  We present an end-to-end algorithm for detecting periodic motion in videos, and counting repeated actions in a sequence.
			</p>
		</div>
    	</div>	
	
	<div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/value_dice.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Imitation Learning via Off-Policy Distribution Matching</b><br>
			  <em>Ilya Kostrikov, Ofir Nachum, Jonathan Tompson</em><br/>
                          ICLR 2020 <a href="https://arxiv.org/abs/1912.05032">[paper]</a><br/>
			  Our ValueDice algorithm improves upon <a href="https://arxiv.org/abs/1809.02925">DAC</a>, enabling similar performance in an entirely offline setting. This is done via a novel formulation of phrasing distributional matching as value function learning problem.
			</p>
		</div>
    	</div>
	
	<div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/adail.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>ADAIL: Adaptive Adversarial Imitation Learning</b><br>
			  <em>Yiren Lu, Jonathan Tompson</em><br/>
			  NeurIPS 19 workshop + Submission
			  <a href="others/ADAIL_NeurIPS_2019_Workshop_Camera_ready.pdf">[paper]</a><br/>
			  The ADAIL tackles the problem of adaptive policies learning. We use an explicit latent dynamics encoder to improve policy robustness to unseen dynamics.
			</p>
		</div>
    </div>
	
	<div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/tracking.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>An Analysis of Object Representations in Deep Visual Trackers</b><br>
			  <em>Jonathan Tompson*, Ross Goroshin*, Debidatta Dwibedi (* equal)</em><br/>
			  Submission
			  <a href="https://arxiv.org/abs/2001.02593">[paper]</a><br/>
			  In this work we performed an in-depth analysis of a popular tracking architecture and suggested a novel architecture to mitigate saliency biases.
			</p>
		</div>
    </div>
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
			<img class="paper-image" src="images/top.gif" alt="">
		    <!--<video class="paper-image" src="images/top.mp4" autoplay loop playsinline muted></video>-->
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Temporal Cycle-Consistency Learning</b><br>
			  <em>Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman</em><br/>
			  CVPR 2019
			  <a href="https://arxiv.org/abs/1904.07846">[paper]</a>
			  <a href="https://sites.google.com/view/temporal-cycle-consistency/">[webpage]</a>
			  <a href="https://www.youtube.com/watch?v=iWjjeMQmt8E">[video]</a><br/>
			  A self-supervised representation learning method based on the task of temporal alignment between videos. In addition to robustly solving video alignment, these representations enable few-shot classification of video action phases.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/8tasks1920x540_cropped.gif" alt="">
		    <!--<video class="paper-image" src="images/8tasks1920x540_cropped.mp4" autoplay loop playsinline muted></video>-->
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Learning Latent Plans from Play</b><br>
			  <em>Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet </em><br/>
			  CORL 2019
			  <a href="https://arxiv.org/pdf/1903.01973.pdf">[paper]</a>
			  <a href="https://learning-from-play.github.io/">[website]</a><br/>
			  A novel method for learning hierarchical robotic control policies from unstructured play data.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/dac_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning</b><br>
			  <em>Ilya Kostrikov, Kumar Krishna Agrawal , Debidatta Dwibedi, Sergey Levine, Jonathan Tompson</em><br/>
			  ICLR 2019
			  <a href="https://arxiv.org/abs/1809.02925">[paper]</a>
			  <a href="https://github.com/google-research/google-research/tree/master/dac">[code]</a><br/>
			  Presented a SoTA Adversarial Imitation Learning method that utilizes an off-policy variant of the <a href="https://arxiv.org/abs/1606.03476">GAIL</a> algorithm, as well as a novel mechanism for handling absorbing states.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/person_lab.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>PersonLab: Person Pose Estimation and Instance Segmentation</b><br>
			  <em>George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, Kevin Murphy</em><br/>
			  ECCV 2018
			  <a href="https://arxiv.org/abs/1803.08225">[paper]</a><br/>
			  A box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/mftcn.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Learning Actionable Representations from Visual Observations</b><br>
			  <em>Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet</em><br/>
			  IROS 2018
			  <a href="https://arxiv.org/abs/1808.00928">[paper]</a>
			  <a href="https://sites.google.com/view/actionablerepresentations/">[website]</a><br/>
			  A novel framework for learning robust visual features for training robotic agents. Building upon the <a href="https://arxiv.org/abs/1704.06888" class="link">TCN</a> framework, we show that our learned representations are as performant as policies trained from true state representations.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/debi_net.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Temporal Reasoning in Videos using Convolutional Gated Recurrent Units</b><br>
			  <em>Debidatta Dwibedi, Jonathan Tompson, Pierre Sermanet</em><br/>
			  CVPR 2018 Workshop
			  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w19/Dwibedi_Temporal_Reasoning_in_CVPR_2018_paper.pdf">[paper]</a><br/>
			  An architecture for video-based action recognition using a novel latent prediction loss to constrain and improve latent representations.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="two columns" style="margin-top: 2%; width: 12.61111%">
		    <img class="paper-image" src="images/plane_spin.gif" alt=""><br>
			<img class="paper-image" src="images/expand.gif" alt="">
        </div>
        <div class="two columns" style="margin-top: 2%; margin-left: 0.75%; width: 12.61111%">
		    <img class="paper-image" src="images/car_spin.gif" alt=""><br>
			<img class="paper-image" src="images/wig1.gif" alt="">
        </div>
        <div class="two columns" style="margin-top: 2%; margin-left: 0.75%; width: 12.61111%">
		    <img class="paper-image" src="images/chair_spin.gif" alt=""><br>
			<img class="paper-image" src="images/wig2.gif" alt="">
        </div>		
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Discovery of Semantic 3D Keypoints via End-to-end Geometric Reasoning</b><br>
			  <em>Supasorn Suwajanakorn. Noah Snavely, Jonathan Tompson, Mohammad Norouzi</em><br/>
			  NIPS 2018 Oral
			  <a href="https://arxiv.org/pdf/1807.03146.pdf">[paper]</a>
			  <a href="https://keypointnet.github.io/">[website]</a><br/>
			  A semi-supervised method to recover semantically consistent 3D keypoints from weakly labeled RGB data. 
			</p>
		</div>
    </div>
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/scoop_gif.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Learning Robotic Manipulation of Granular Media</b><br>
			  <em>Connor Schenck, Jonathan Tompson, Dieter Fox, Sergey Levine</em><br/>
			  CoRL 2017
			  <a href="https://arxiv.org/abs/1709.02833">[paper]</a>
			  <a href="https://www.youtube.com/watch?v=mpjwW618n_Y">[video]</a><br/>
			  This paper examines the problem of robotic manipulation of graunular media, where we learn predictive models of granular media dynamics to perform scooping and dumping actions.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/vale_gif_compressed.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Towards Accurate Multi-person Pose Estimation in the Wild</b><br>
			  <em>George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy</em><br/>
			  CVPR 2017
			  <a href="https://arxiv.org/abs/1701.01779">[paper]</a>
			  <a href="https://drive.google.com/open?id=0By7jnMwv2HCOUDJWV3ZZM3YwTUk">[slides]</a><br/>
			  State-of-the-art RGB human pose on MSCOCO using a 2-stage system for top-down detection.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/arch_gif_big.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Accelerating Eulerian Fluid Simulation With Convolutional Networks</b><br>
			  <em>Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin</em><br/>
			  ICML 2017
			  <a href="http://arxiv.org/abs/1607.03597">[paper]</a>
			  <a href="https://www.youtube.com/watch?v=w71zxkniJfo">[video]</a>
			  <a href="https://vimeo.com/238220956">[video presentation]</a>
			  <a href="http://cims.nyu.edu/~schlacht/CNNFluids.htm">[website]</a><br/>
			  A learning-based system for simulating Navier-Stokes Equations in real-time. We do so by reformulating the standard operator splitting method as an end-to-end network.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/merlin_small_fast.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Inside-out Hand Tracking</b><br>
			  <!-- <em>Jonathan Tompson, et al.</em><br/> -->
			  Google Daydream 2016 <br/>
			  <!-- <a href="images/merlin_video.mp4">[video]</a><br/> -->
			  Lead a project to enable high quality hand-tracking from a head-mounted camera. The ConvNet-based system ran in real time on embedded hardware. The system demonstrated robustness to occlusion and hand-shape variation. More details are unfortunately not yet public.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/thesis_snippit.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>PhD Thesis: Localization of Humans in Images Using Convolutional Networks</b><br>
			  NYU 2015
			  <a href="others/thesis.pdf">[thesis]</a>
			  <a href="others/thesis-defense_no_videos.pdf">[ppt]</a>
			  <br/>
			  My PhD thesis covers *most* of the human body tracking work I did while at NYU.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/mocap.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Efficient ConvNet-based Marker-less Motion Capture in General Scenes with a Low Number of Cameras</b><br>
			  <em>Ahmed Elhayek, Edilson De Aguiar, Arjun Jain, Jonathan Tompson, Leonid Pishchulin, Micha Andriluka, Christoph Bregler, Bernt Schiele, Christian Theobalt</em><br/>
			  CVPR 2015
			  <a href="others/cvpr2015_ahmed.pdf">[paper]</a>
			  <a href="http://gvv.mpi-inf.mpg.de/projects/convNet_moCap/vids/video.mp4">[video]</a>
			  <a href="http://gvv.mpi-inf.mpg.de/projects/convNet_moCap/">[website]</a><br/>
			  SoTA motion capture in arbitrary scenes from few cameras.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/cvpr_2014_model.jpg" alt=""><br>
			<img class="paper-image" src="images/cvpr_2014_detections.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Efﬁcient Object Localization Using Convolutional Networks</b><br>
			  <em>Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christoph Bregler</em><br/>
			  CVPR 2015
			  <a href="others/tompson_cvpr15.pdf">[paper]</a>
			  <a href="data/flic_predictions_cvpr.zip">[predictions_flic]</a>
			  <a href="data/mpii_test_pred.zip">[predictions_mpii]</a><br/>
			  A novel cascaded architecture to help overcome the effects of MaxPooling and a modified dropout that works better in the presence of spatially-coherent activations. Achieved SoTA in human body tracking.<br>
			  Turked MPII images containing one person: <a href="data/single_person_list_MPII.txt">[data]</a>.
			</p>
		</div>
    </div>		
	
	<!--<div class="row" style="background: url(images/img02.gif) repeat-x left top;">-->
	
	<!--
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/deep_patch_neighbours.jpg"><br>
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Local Image Descriptor</b><br>
			  <em>Arjun Jain, Jonathan Tompson, Christoph Bregler</em><br/>
			  Designed a SIFT-like descriptor, which outperforms all existing state-of-the-art.<br>
			</p>
		</div>
    </div>
	-->
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/flic_plus_sample_cropped.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>FLIC-plus Dataset</b><br>
			  <em>Jonathan Tompson, Arjun Jain, Christoph Bregler, Yann LeCun</em><br/>
			  NIPS 2014
			  <a href="flic_plus.htm">[website]</a><br/>
			  Cleaned up an filtered the FLIC Human Pose dataset of Sapp et al. for fairer evaluation and higher quality labels.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/LSP_results.jpg" alt=""><br>
			<img class="paper-image" src="images/nips_2014_arch1.jpg" alt=""><br>
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation</b><br>
			  <em>Jonathan Tompson, Arjun Jain, Christoph Bregler, Yann LeCun</em><br/>
			  NIPS 2014
			  <a href="others/joint-training-convolutional.pdf">[paper]</a>
			  <a href="data/flic_lsp_predictions.zip">[predictions]</a><br/>
			  Following ICLR 2014 work, we substantially improved the architecture, incorporated the MRF into the ConvNet and significantly outperformed existing SoTA.
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/motion_feat.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation</b><br>
			  <em>Arjun Jain, Jonathan Tompson, Yann LeCun, Christoph Bregler</em><br/>
			  ACCV 2014
			  <a href="http://arxiv.org/pdf/1409.7963v1.pdf">[paper]</a><br/>
			  For ambiguous poses with poor image evidence (such as detecting the pose of camouflaged actors), we showed that motion flow features allow us to outperform state-of-the-art techniques.
			</p>
		</div>
    </div>
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/features.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Unsupervised Feature Learning from Temporal Data</b><br>
			  <em>Rostislav Goroshin, Joan Bruna, Jonathan Tompson, Arthur Szlam, David Eigen, Yann LeCun</em><br/>
			  ACCV 2014
			  <a href="others/nips2014_workshop_ross.pdf">[paper]</a><br/>
			  A sparse auto-encoder architecture to make use of temporal coherence. This formulation enables pre-training on unlabeled video data (of which there is a massive abundance), to improve ConvNet performance.
			</p>
		</div>
    </div>	
	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/body_tracking_2_2.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Learning Human Pose Estimation Features with Convolutional Networks</b><br>
			  <em>Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor, Christoph Bregler</em><br/>
			  ICLR 2014
			  <a href="others/iclr2014_paper.pdf">[paper]</a><br/>
			  It was a new architecture for human pose estimation using a ConvNet + MRF spatial model and it was the first paper to show that a variation of deep learning could outperform existing architectures.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/nyu_hand_pose_dataset_sample2.png" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>NYU Hand Pose Dataset</b><br>
			  <em>Jonathan Tompson, Murphy Stein, Ken Perlin, Yann LeCun</em><br/>
			  <a href="NYU_Hand_Pose_Dataset.htm" class="link">[website]</a> <a href="https://github.com/jonathantompson/ModelFit">[code]</a><br/>
			  High quality hand pose dataset released. Was the primary hand pose evaluation dataset for the community for years.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/hand_tracking.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks</b><br>
			  <em>Jonathan Tompson, Murphy Stein, Ken Perlin, Yann LeCun</em><br/>
			  SIGGRAPH 2014
			  <a href="others/TOG_2014_paper.pdf">[paper]</a>
			  <a href="https://www.youtube.com/watch?v=J4c_x1QnW0A">[video]</a>
			  <a href="others/paper_presentation_no_videos.pptx">[ppt]</a>
			  <a href="https://github.com/jonathantompson/KinectHands">[code]</a><br/>
			  A novel method for real-time pose recovery of markerless complex articulable objects from a single depth image. We showed state-of-the-art results for real-time hand tracking.<br>
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/dlock2.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Distributed Locking Protocol</b><br>
			  Working at MongoDB Inc with the server kernel team (under Alberto Lerner): developed a new distributed lease protocol (for the sharding config server) using a heavily modified 2-phase commit with timeout mechanism.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/rdf_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Open-Source Randomized Decision Forests</b><br>
			  <a href="https://www.youtube.com/watch?v=fj7nafr39rU">[video]</a> <a href="https://github.com/jonathantompson/KinectHands">[code]</a><br/>
			  Early hand-tracking research: a randomized-decision-forest classifier trained to recognize hand pixels using Microsoft's Kinect. Provides an OSS implementation of <a href="http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf">Real-Time Human Pose Recognition in Parts from Single Depth Images</a>
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		    <img class="paper-image" src="images/arcade_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>ARCADE</b><br>
			  <em>Jonathan Tompson, Ken Perlin, Murphy Stein, Charlie Hendee, Xiao Xiao Hiroshi Ishii</em><br/>
			  SIGGRAPH Realtime Live 2012 <a href="https://www.youtube.com/watch?v=JP9AIjokW_k">[video]</a><br/>
			  Group project with the MIT Media Lab and NYU Media Research Lab. ARCADE is a system that allows real-time video-based presentations that convey the illusion that presenters are directly manipulating holographic 3D objects with their hands. 
			</p>
		</div>
    </div>		
	
	<!--
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
          <img class="paper-image" src="images/prenderer.gif">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p style="margin-bottom: 0.9rem">
			    <b>Open-Source OpenGL Renderer: PRenderer</b><br>
				<a href="https://github.com/jonathantompson/prenderer">[source]</a><br>
				<ul style="margin-bottom: 0.1rem;font-size: 0.75em; list-style-position: outside; margin-left: 1em;">
                    <li>Compact G-Buffer encoding</li>
                    <li>Light volume optimizations</li>
                    <li>Soft shadows using PSVSM</li>
                    <li>Screen space ambient occlusion</li>
                    <li>Motion blur</li>
                    <li>HDR Rendering pipeline</li>
                    <li>Depth of Field</li>
				</ul>
			</p>
		</div>
    </div>
    -->	
	
    <div class="row">
        <div class="two columns" style="margin-top: 2%; width: 19%">
          <img class="paper-image" src="images/obb_tree_cropped.gif" alt="">
		  <img class="paper-image" src="images/cloth_sim_cropped.gif" alt="">
        </div>
        <div class="two columns" style="margin-top: 2%; margin-left: 1.33333%; width: 19%">
          <img class="paper-image" src="images/mesh_dec_bunny_cropped.gif" alt="">
		  <!-- <img class="paper-image" src="images/hungry_bee_crop.gif"> -->
		  <img class="paper-image" src="images/prenderer_cropped.gif" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p style="margin-bottom: 0.5rem">
			    <b>Miscellaneous Open-Source Graphics Projects</b><br>
				Over the years I have open sourced implementations of multiple graphics algorithms, including:
				<ul style="margin-bottom: 0.5rem;font-size: 1.0em; list-style-position: outside; margin-left: 1em;">
				  <li>OSS implementation of <a href="ftp://ftp.cs.unc.edu/pub/users/manocha/PAPERS/COLLISION/sig96.pdf">OBBTree: A Hierarchical Structure for Rapid Interference Detection</a> <a href="http://code.google.com/p/obbdetection/">[source]</a></li>
				  <li> OSS implementation of <a href="http://research.microsoft.com/en-us/um/people/hoppe/newqem.pdf">New Quadric Metric for Simplifying Meshes with Appearance Attributes</a></li>
				  <li>OSS implementation of <a href="http://www.cs.cmu.edu/~baraff/papers/sig98.pdf">large steps in cloth simulation</a> <a href="http://code.google.com/p/clothsim/">[source]</a></li>
				  <li>Open-Source OpenGL Renderer: PRenderer <a href="https://github.com/jonathantompson/prenderer">[source]</a></li>	
				  <li>An XNA Game "Hungry Bee" game development tutorial <a href="http://code.google.com/p/hungrybee/">[source]</a></li>			  
				</ul>
			</p>
		</div>
    </div>	
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
          <img class="paper-image" src="images/Epoch-sign.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p style="margin-bottom: 0.5rem">
			    <b>IC Reserach and Development at <a href="http://www.epochmicro.com/">Epoch Microelectronics</a></b><br>
				Designed a wide range of mixed signal RFICs including:
				<ul style="margin-bottom: 0.5rem; font-size: 0.8em">
				  <li>RF Modulators (passive and active).</li>
				  <li>Fractional and integer PLLs.</li>
				  <li>Wideband, low-noise LNAs.</li>
				  <li>Low-noise bandgaps and regulators.</li>
				  <li>LC and XTAL oscillators.</li>
				  <li>ΣΔ modulators.</li>
				  <li>FIR/IIR interpolation and decimation filters.</li>
				  <li>Anti-aliasing filters for data converters.</li>
				  <li>Continuous time, high linearity analog filters for base-band processing.</li>
				</ul>
				Worked with multiple telecommunication standards, including:
				<ul style="margin-bottom: 0.5rem;font-size: 0.8em">
				  <li>Cellular: GSM (EDGE), CDMA, WCDMA, LTE, WIMAX </li>
				  <li>Terrestrial and Cable Television: DVB, ATSC, ISDB </li>
				  <li>Low-power standards: Bluetooth, Zigbee </li>
				</ul>
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
          <img class="paper-image" src="images/PK_Contactless_Chip_cropped.jpg" alt="">
		  <!--<img class="paper-image" src="images/PK_Contactless_Architecture.jpg">-->
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>2.6GHz RF Inductive Power Delivery for Contactless On-Wafer Characterization</b><br>
			  <em>Jonathan Tompson, Adam Dolin and Peter Kinget</em><br/>
			  ICMTS 2008 <a href="others/wireless_tompson.pdf">[paper]</a><br/>
			  Designed a contactless IC testing mechanism using inductive probing through custom devices.
			</p>
		</div>
    </div>		
	
    <div class="row">
        <div class="five columns" style="margin-top: 2%">
		  <img class="paper-image" src="images/PK_test_setup_cropped.jpg" alt="">
		  <!--<img class="paper-image" src="images/PK_Variation_Chip.jpg" alt="">-->
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
			  <b>Mismatch Characterization of Ring-Oscillators</b><br>
			  <em>Jonathan Tompson, Peter Kinget</em><br>
			  2008 Research Associate<br>
			  Investigated the matching of on-chip oscillators and compared statistics to theoretical estimates.
			</p>
		</div>
    </div>	

    <div class="row">
        <div class="five columns" style="margin-top: 2%">
          <!-- <img class="paper-image" src="images/spiral_ind.jpg" alt=""> -->
		  <img class="paper-image" src="images/spiral_ind2.jpg" alt="">
        </div>
        <div class="seven columns" style="margin-top: 2%">
			<p>
				<b>High-Speed, Chip-to-Chip Communcation</b><br>
				<em>Jonathan Tompson, Gu-Yeon Wei</em><br>
				2006 Undergraduate Honors thesis<br>
				Designed a novel transformer-based communication system for high-speed digital systems.
			</p>
		</div>
    </div>
	
    <div class="row">
        <div class="ten columns" style="margin-top: 2%">
			<p style="margin-bottom: 0.5rem">
			  <b>Open Source Tools</b><br>
			  I've written or contributed to many OSS tools over the years.  This is a short list of some:
			  <ul>
				  <li><a href="https://github.com/jonathantompson/jtorch" class="link">jtorch</a> - Torch7 Utility Library for running models in OpenCL / C++</li>
				  <li><a href="https://github.com/jonathantompson/jcl" class="link">jcl</a> - OpenCL Wrapper (to make OpenCL easier)</li>
				  <li><a href="https://github.com/jonathantompson/torchzlib" class="link">torchzlib</a> - A utility library for zlib compression / decompression of Torch7 tensors</li>
				  <li><a href="https://github.com/jonathantompson/matlabnoise" class="link">matlabnoise</a> - Matlab procedural noise library</li>
				  <li><a href="https://github.com/jonathantompson/matlabobj" class="link">matlabobj</a> - Matlab obj reader</li>
				  <li><a href="http://torch.ch/" class="link">torch7</a> - I'm a reasonably regular contributor to torch and it's various packages </li>
				  <li><a href="https://github.com/jonathantompson/icp" class="link">icp</a> - A C++ Iterative Closest Point library (with Matlab interface)</li>
				  <li><a href="https://github.com/jonathantompson/ik" class="link">ik</a> - A very simple inverse kinematics library (in C++) </li>
				  <li><a href="https://github.com/jonathantompson/ModelFit" class="link">ModelFit</a> - Off-line fitting portion of the hand-tracking paper below</li>
				  <li><a href="https://github.com/jonathantompson/jzmq" class="link">jzmq</a> - A ZeroMQ Utility Library (C++)</li>
				  <li> There are probably others...  See my <a href="https://github.com/jonathantompson/" class="link">github</a>.</li>
			  </ul>
			</p>
		</div>
    </div>	
	
  </div>
  
  <div class="container">

    <div class="row" style="margin-top: 2%" id="resume">
      <h4>Resume</h4>
    </div>
	
	<div class="row">
	    <p style="margin-bottom: 0rem">
        
            <!--
            <div>
                <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/pfd.gif" alt="" width="30" height="30"> </p>
                <a class="project-link" href="resume/JJRT_resume_no_address.pdf" target="_blank">PDF</a><br>
            </div>
            <div style="clear: left;">
                <p style="float: left; margin-bottom: 4px; margin-right: 10px"> <img src="images/word.gif" alt="" width="30" height="30"> </p>
                <a href="resume/JJRT_resume_no_address.doc">WORD</a><br>
            </div>            
            -->
            
	        <a href="resume/JJRT_resume_no_address.pdf" style="text-decoration: none"><img  src="images/pfd.gif" height="30" alt=""/> </a><a href="resume/JJRT_resume_no_address.pdf">PDF</a> <br/>
	        <a href="resume/JJRT_resume_no_address.doc" style="text-decoration: none"><img  src="images/word.gif" height="30" alt=""/> </a><a href="resume/JJRT_resume_no_address.doc">WORD</a>
	        
            <br/>
	    </p>
	    <div class="twelve columns" style="margin-top: 2%">
		    <!--Used https://www.zamzar.com/convert/pdf-to-svg/ to convert pdf to svg-->
		    <img class="paper-image" src="resume/JJRT_resume_no_address_page_01.svg" style="Border: 1px solid black;" alt="">
			<img class="paper-image" src="resume/JJRT_resume_no_address_page_02.svg" style="Border: 1px solid black;" alt="">
            <img class="paper-image" src="resume/JJRT_resume_no_address_page_03.svg" style="Border: 1px solid black;" alt="">
		    <!--<embed src="resume/JJRT_resume_no_address.pdf" width="500" height="375" type="application/pdf">-->
	        <!--<iframe src="resume/JJRT_resume_no_address.pdf" 
style="width:900px; height:2150px;" frameborder="0">Embedded pdf not supported by your browser.  Please click the download link above.</iframe>-->
	    </div>
		<br/>
	</div>
	
  </div>	

<br>
<br>
<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->


</body></html>
